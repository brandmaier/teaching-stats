---
title: "Multiple Regression"
subtitle: "Bachelor-Studiengang Psychologie, 2.FS; Materialien:\_https://github.com/brandmaier/teaching-stats"
author: "Andreas M. Brandmaier"
#institute: "Max Planck Institute for Human Development"
date: "16.12.2019"
output: 
  beamer_presentation:
    latex_engine: xelatex
    theme: metropolis
    includes:
      in_header: mystyle.tex
classoption: "aspectratio=169"

---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, out.height = "90%", cache=TRUE)
require(ggplot2)

mytheme <- ggthemes::theme_base()
#mytheme <- theme_minimal()


##Cohen (2003) Table 3.5.1
cohen.dat <- data.frame(
    salary = c(51876, 54511, 53425, 61863, 52926, 47034, 66432, 61100, 41934,
      47454, 49832, 47047, 39115, 59677, 61458, 54528, 60327, 56600,
      52542, 50455, 51647, 62895, 53740, 75822, 56596, 55682, 62091,
      42162, 52646, 74199, 50729, 70011, 37939, 39652, 68987, 55579,
      54671, 57704, 44045, 51122, 47082, 60009, 58632, 38340, 71219,
      53712, 54782, 83503, 47212, 52840, 53650, 50931, 66784, 49751,
      74343, 57710, 52676, 41195, 45662, 47606, 44301, 58582),
pubs =c(18,3,2,17,11,6,38,48,9,22,30,21, 10, 27, 37, 8, 13, 6, 12, 29, 29, 7, 6, 69, 11, 9, 20, 41, 3, 27, 14, 23, 1, 7, 19, 11, 31, 9, 12, 32, 26, 12, 9, 6, 39, 16, 12, 50, 18, 16, 5, 20, 50,
      6, 19, 11, 13, 3, 8, 11, 25, 4),
    cits = c(50, 26, 50, 34, 41, 37, 48, 56, 19, 29,
        28, 31, 25, 40, 61, 32, 36, 69, 47, 29, 35,
        35, 18, 90, 60, 30, 27, 35, 14, 56, 50, 25,
        35, 1, 69, 69, 27, 50, 32, 33, 45, 54, 47, 29,
        69, 47, 43, 55, 33, 28, 42, 24, 31, 27,
        83, 49, 14, 36, 34, 70, 27, 28),
sex= c(1,1,1,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,
  0,1,0,0,1,1,1,1,0,1,1,1,1,1)
)
```

## Wann benötigen wir die multiple Regression?

Frage: Wie groß ist der Einfluss *mehrerer* metrischer oder kategorialer Prädiktoren auf ein metrisches Kriterium?

![](pathdiagram.pdf)

## Zentrale Ansätze

> - Erklären: Hängen die Prädiktoren mit dem Kriterium zusammen? \newline Wenn ja, wie stark? Wie gut erklären sie das Kriterium?
> - Vorhersagen: Wie gut können wir das Kriterium (in neuen Daten) vorhersagen?

## Ein einfaches Modell

Ein einfaches Erklärungs- und Prognose-Modell: Wir sagen für jede Person den Mittelwert von Gehalt vorher (ohne die Prädiktoren zu kennen)

\center

```{r fig.height=3.5, fig.width=5, out.height="72%"}
id <-45
ymax <- cohen.dat[id,"salary"]
lm.dumb<- lm(salary~1, data=cohen.dat)
ymin <- coef(lm.dumb)[1]
ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
    geom_abline(intercept=coef(lm.dumb),slope=0,lwd=2,color="blue")+
    geom_linerange(lty=2,x=cohen.dat[id,"pubs"],ymin=ymin,ymax=ymax,col="red")+
  mytheme#+
  #ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")


```

## Konstante Regression

Ein formales Modell für die beobachteten Daten $i \in 1..N$ mit $N$ als Stichprobengröße:

\beginlarge
$$ y_i = \color{blue}b_0 + \color{red}e_i$$
\endlarge

- $y_i$: Wert von Person $i$ im Kriterium
- $\color{blue}b_0$: Regressionskonstante 
- $e_i$: Vorhersagefehler für Person $i$ (Residuum)

## Ein verbessertes Vorhersagemodell

\center
```{r fig.height=3.5, fig.width=5}
lm.pubs <- lm(salary~pubs, data=cohen.dat)

lm.pubs <- lm(salary~pubs, data=cohen.dat)
df.test <- data.frame(pubs = 60)
df.test$salary <- predict(lm.pubs, df.test)

ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
    geom_smooth(method = "lm",se=FALSE,lwd=2)+
 #   geom_point(data=df.test,size=8,color='blue',pch=1)+
 # geom_segment(x=60,xend=60,y=0,yend=df.test$salary,lty=2,col="grey")+
#  geom_segment(x=0,xend=60,y=df.test$salary,yend=df.test$salary,lty=2,col="grey")+
  mytheme#+
  #ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")


```


## Einfache Regression

Einfache Regressionsgleichung für einen Prädiktor $X$ und die metrische Kriteriumsvariable $Y$:

\beginlarge
$$ y_i = \color{blue}b_0\color{black} + \color{blue}b_1\color{black} \cdot x_i + \color{red}{e_i}$$\normalsize
\endlarge

- $y_i$: Wert von Person $i$ im Kriterium
- $x_{i,j}$: Wert von Person $i$ in dem Prädiktor $X$
- $\color{red}e_i$: Vorhersagefehler für Person $i$ (Residuum)
- $\color{blue}b_0$: Regressionskonstante 
- $\color{blue}b_1$: Regressionskoeffizient


## Interpretation der Koeffizienten

\begincols
\begincol{.38\textwidth}

- *Regressionskonstante*: \newline Wert des Kriteriums, wenn der Prädiktor den Wert 0 annimmt.
- *Regressionskoeffizienten* (auch Regressionsgewichte): Erwartete Veränderung des Kriteriums $Y$, wenn
man den Prädiktor $X$ um eine Einheit erhöht.

\endcol
\begincol{.58\textwidth}

```{r fig.height=3.5, fig.width=5}
lm.pubs <- lm(salary~pubs, data=cohen.dat)

lm.pubs <- lm(salary~pubs, data=cohen.dat)
df.test <- data.frame(pubs = 0)
df.test$salary <- predict(lm.pubs, df.test)
df.test2 <- data.frame(pubs = 20)
df.test2$salary <- predict(lm.pubs, df.test2)
ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
  geom_smooth(method = "lm",se=FALSE)+
  geom_point(data=df.test,size=8,color='blue',pch=1)+
  geom_segment(x=20,xend=20,y=coef(lm.pubs)[1],yend=df.test2$salary,lty=2,col="blue")+
  geom_segment(x=0,xend=20,y=coef(lm.pubs)[1],yend=coef(lm.pubs)[1],lty=2,col="blue")+
  mytheme#+
#  ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")
```
\endcol
\endcols

## Interpretation der Koeffizienten

\begincols
\begincol{.38\textwidth}

- *Regressionskonstante*: $b_0=48439$ 
- *Regressionskoeffizienten* (auch Regressionsgewichte): $b_1=351$
- $20\cdot351 = 7020$

\endcol
\begincol{.58\textwidth}

```{r fig.height=3.5, fig.width=5}
lm.pubs <- lm(salary~pubs, data=cohen.dat)

lm.pubs <- lm(salary~pubs, data=cohen.dat)
df.test <- data.frame(pubs = 0)
df.test$salary <- predict(lm.pubs, df.test)
df.test2 <- data.frame(pubs = 20)
df.test2$salary <- predict(lm.pubs, df.test2)
ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
  geom_smooth(method = "lm",se=FALSE)+
  geom_point(data=df.test,size=8,color='blue',pch=1)+
  geom_segment(x=20,xend=20,y=coef(lm.pubs)[1],yend=df.test2$salary,lty=2,col="blue")+
  geom_segment(x=0,xend=20,y=coef(lm.pubs)[1],yend=coef(lm.pubs)[1],lty=2,col="blue")+
  mytheme#+
#  ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")
```
\endcol
\endcols


## Multiple Regression

Frage: Wie groß ist der Einfluss *mehrerer* (metrischer oder kategorialer) Prädiktoren auf ein metrisches Kriterium?

\beginlarge
$$\color{black}\Huge y_i = \color{blue}b_0 + \color{blue}b_1\color{black} \cdot x_{i,1} +\color{blue} b_2\color{black} \cdot x_{i,2} + \ldots + \color{blue}b_k \color{black} \cdot x_{i,k}+\color{red} e_i$$
\endlarge

- $y_i$: Wert von Person $i$ im Kriterium
- $x_{i,j}$: Wert von Person $i$ in dem Prädiktor $j$
- $\color{red} e_i$: Vorhersagefehler für Person $i$ (Residuum)
- $\color{blue}  b_0$: Regressionskonstante (Intercept)
- $\color{blue}  b_j$: Regressionsgewichte ($j=1\ldots k$)

## Beispiel ($k=2$)

\center

```{r  fig.height=4, fig.width=4, out.width="85%", out.height="85%"}
library(scatterplot3d) # This library will allow us to draw 3d plot

id <- 45


  temp <- cohen.dat 
  temp$salary <- temp$salary/1000
  
  ymax <- temp[id,"salary"]
my.lm<- lm(salary~pubs+cits, data=temp)
ymin <- predict(my.lm)[id]
  
  plot3d <- scatterplot3d(temp$pubs, temp$cits, temp$salary,angle=55, scale.y=0.7, pch=16,
                        xlab="X: Publikationen", ylab="Y: Zitationen", zlab="Z: Gehalt [T-USD]")

  my.lm<- lm(salary ~ pubs + cits, temp)
  plot3d$plane3d(my.lm, lty.box = "solid", col="blue")
  plot3d$points3d(x = c(cohen.dat[id,"pubs"],cohen.dat[id,"pubs"]) ,
                  y=c(cohen.dat[id,"cits"],cohen.dat[id,"cits"]),
                  z=c(ymin,ymax), col="red", type="l",lwd=3)



my.lm<- lm(salary ~ pubs + cits, cohen.dat)

```

## Regressionskoeffizient

\center
Ebene: $\mathrm{Gehalt} = \color{blue}40493\color{black} + \color{black}252\color{black} \cdot Publikationen + \color{black} 242\color{black} \cdot Zitationen$
\begincols
  \begincol{.38\textwidth}
  


$\color{blue}b_0$ gibt an, welcher Wert zu erwarten ist, falls sämtliche Prädiktoren den Wert Null realisieren.


  \endcol
\begincol{.58\textwidth}

```{r message=FALSE, echo=FALSE, warning=FALSE, results=FALSE}
  
  temp <- cohen.dat 
  temp$salary <- temp$salary/1000
  
  pdf("temp-3d.pdf", width=4, height=4)
  
  plot3d <- scatterplot3d(temp$pubs, temp$cits, temp$salary,angle=55, scale.y=0.7, pch=16,
                        xlab="X: Publikationen", ylab="Y: Zitationen", zlab="Z: Gehalt [T-USD]")

  my.lm<- lm(salary ~ pubs + cits, temp)
  plot3d$plane3d(my.lm, lty.box = "solid")
plot3d$points3d(0,0,coef(my.lm)[1],col="blue",pch=16,cex=3)

dev.off()
```

![](temp-3d.pdf)

\endcol
\endcols

## Interpretation der Koeffizienten

\center
Ebene: $\mathrm{Gehalt} = \color{black}40493\color{black} + \color{blue}252\color{black} \cdot Publikationen + \color{black} 242\color{black} \cdot Zitationen$

\begincols
\begincol{.38\textwidth}
  
- Die $\color{blue} b_i$ geben an, um wieviel sich der Wert von dem Kriterium ($y$) ändert, wenn man den zugehörigen $i$-ten Prädiktor um 1 Einheit verändert *und* man alle übrigen Prädiktoren konstant hält.

- Daher bezeichnen wir diese nun als _partielle Regressionskoeffizienten_.


\endcol
\begincol{.58\textwidth}

```{r  message=FALSE, echo=FALSE, warning=FALSE, results=FALSE}
pdf("temp-3d-2.pdf", width=4, height=4)
  
plot3d <- scatterplot3d(temp$pubs, temp$cits, temp$salary,angle=55, scale.y=0.7, pch=16,
                        xlab="X: Publikationen", ylab="Y: Zitationen", zlab="Z: Gehalt [T-USD]")

my.lm<- lm(salary ~ pubs + cits, temp)
plot3d$plane3d(my.lm, lty.box = "solid",col="black")
#plot3d$points3d(40,60,coef(my.lm)[1]+40*coef(my.lm)[2]+60*coef(my.lm)[2],col="red",pch=16,cex=3)
plot3d$points3d(c(0,70),c(0,0),c(coef(my.lm)[1],coef(my.lm)[1]+70*coef(my.lm)[2]), col="blue",type="l",lwd=2)
offs <- 60
plot3d$points3d(c(0,70),c(offs,offs),c(coef(my.lm)[1]+offs*coef(my.lm)[3],coef(my.lm)[1]+70*coef(my.lm)[2]+offs*coef(my.lm)[3]), col="blue",type="l",lwd=2)

dev.off()

```

![](temp-3d-2.pdf)

\endcol
\endcols

## Fehler

Wie schätzen wir die Parameter? Dazu betrachten wir die Fehler in den beobachteten Werten:

\center

```{r fig.height=3, fig.width=5, out.height="75%",dpi=180}
id <-45
lm.pubs <- lm(salary~pubs, data=cohen.dat)
ymin <- predict(lm.pubs)[id]
ymax <- cohen.dat[id,"salary"]

ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
  geom_smooth(method = "lm",se=FALSE)+
  geom_linerange(lty=2,x=cohen.dat[id,"pubs"],ymin=ymin,ymax=ymax,col="red")+
  geom_text(x=cohen.dat[id,"pubs"]+5,y=(ymax+ymin)/2+5000,label="Fehler",col="red")+
  mytheme


```

## Schätzung: Methode der kleinsten Quadrate

- Idee: Das beste Regressionsmodell soll das sein, das die Streuung der Fehler minimiert

- Wir nennen das die Residuenquadratsumme oder auch Summe der quadrierten Residuen (SQR)

$$SQR=\sum^n_{i=1}{e^2_i}=\sum^n_{i=1}\left[y_i-(b_0+b_1x_{i,1}+\ldots+b_kx_{i,k})^2\right]$$

- Wähle die Koeffizienten so, dass wir die Summe der quadrierten Fehlerterme  minimieren
- Exakt und effizient lösbar (falls lösbar)

##  Methode der kleinsten Quadrate (Schematisch)

\center

```{r out.height="80%",fig.height=4, fig.width=6}
plot_kq <- function(intercept=NULL, slope=NULL) {
  
model <- lm(salary~pubs, data = cohen.dat)
if (!is.null(intercept)) { model$coefficients[1]<-intercept }
if (!is.null(slope)) { model$coefficients[2]<-slope }

scf <- sqrt(var(cohen.dat$salary))/sqrt(var(cohen.dat$pubs))

y.hat<-predict(model)
empty <- rep(NA,nrow(cohen.dat)*4)
df <- data.frame(x=empty,y=empty,grp=empty)
for (i in 1:nrow(cohen.dat)) {
  dist <- y.hat[i]-cohen.dat[i,"salary"]
  disty <- dist
  distx <- dist / scf
  y <- cohen.dat[i,"salary"]
  x <- cohen.dat[i,"pubs"] 
 
  df[ (i-1)*4+1, ] <- c(x,y,i)
  df[ (i-1)*4+2, ] <- c(x-distx,y,i)
  df[ (i-1)*4+3, ] <- c(x-distx,y+disty,i)  
  df[ (i-1)*4+4, ] <- c(x,y+disty,i)  
}
ggplot(cohen.dat, aes(x=pubs,y=salary))+
  
 # geom_smooth(method='lm',se=FALSE)+
 #fillcolor<-'red' #'#FFC43F'
  geom_polygon(data=df,aes(x=x,y=y,group=grp),alpha=.20, fill='red',color='#FFFFFF')+
  geom_point(data=cohen.dat, aes(x=pubs,y=salary))+
   geom_abline(intercept=coef(model)[1],slope=coef(model)[2])+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
  mytheme#+ ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")
}

(plot_kq())
```

##  Methode der kleinsten Quadrate (Schematisch)

\center

```{r out.height="80%",fig.height=4, fig.width=6}
plot_kq(intercept=68000, slope=100)
```


## Modellgüte

- Wie gut klappt das eigentlich? Wie gut beschreiben wir die Daten?
- Die Summe der quadrierten Residuen (SQR) ist ein Maß für die Modellpassungsgüte 
- Je kleiner, desto besser die Passung des Modells zu den Daten
- SQR ist abhängig von der Einheit der Kriteriumsvariable (hier, $USD^2$)
- Standardisieren!

## Streuung und Unsicherheit

- Residuenquadratsumme: gesamte Streuung; Varianz: erwartete Streuung

- Varianz der Kriteriumsvariable: Die erwartete (quadrierte) Abweichung vom Mittelwert. Wenn wir die Kriteriumsvariable raten müssten, wäre das der erwartete (quadrierte) Fehler $\rightarrow$ Maß für Unsicherheit

- Varianz der Fehlerterme: Maß für die verbleibende Unsicherheit in unserem Modell

- Wir standardisieren die Varianz der Fehler (Unsicherheit trotz des Modells) anhand der Varianz der Kriteriumsvariable (Unsicherheit ohne Modell) und erhalten ein prozentuales Maß der erklärten Varianz


## Bestimmtheitsmaß

Determinationskoeffizient  / Bestimmtheitsmaß (Multiples $R^2$):
  
  $$ R^2 = \frac{\text{ durch  alle Prädiktoren in  }Y \text{ erklärte Varianz}}{\mathrm{Varianz\ von\ }Y} $$

- Normiert zwischen 0 und 1
- Je höher, desto besser!
- Menschliches Verhalten ist schwer vorherzusagen, daher erwarten wir oft keine hohen Werte (<50%)

## Berechnung

- Berechnung über die Quadratsummen

$$ R^2 = 1- \frac{\sum^n_i{e^2_i}}{\sum^n_i{(y_i-\bar{y})^2}} $$

($\bar{y}$: Mittelwert der $y_i$)
  
## Venn-Diagramm der Varianzen

```{r}
# only code --- no graphics output
library(venn)
cohen.venn <- function(x, ...) {
  venn(x,snames = c("Zitationen","Publikationen","Gehalt"), cexsn=1,...)
}

venn.label <- function(x, txt) {
  centroid <- getCentroid(getZones(x))[[1]]
  text(centroid[1], centroid[2], labels = txt, cex = 0.85)
}
my.lm<- lm(salary ~ pubs + cits, cohen.dat)
smlm <- summary(my.lm)

r2.total <- smlm$r.squared # 42% explained

my.lm.pubs <- lm(salary ~ pubs, cohen.dat)
r2.pubs <- summary(my.lm.pubs)$r.squared # 25%


my.lm.cits <- lm(salary ~ cits, cohen.dat)
r2.cits <- summary(my.lm.cits)$r.squared # 30%

cits.unique <- r2.total-r2.cits
pubs.unique <- r2.total-r2.pubs
r2.joint <- r2.total-pubs.unique-cits.unique
```

Links schattiert: Die gesamte Varianz des Kriteriums (unsere Unsicherheit über das Kriterium) \newline Rechts schattiert: Die nicht-erklärte Varianz (verbleibende Unsicherheit)

\center

\begincols
  \begincol{.38\textwidth}

  \endcol
\endcols

```{r fig.height=4, fig.width=8, out.height="65%"}
par(mfrow=c(1,2))
cohen.venn("001+111+101+011")
cohen.venn("001")
```




## Venn-Diagramm der einfachen $R^2$

- Schattierte Fläche: Aufgeklärte Varianz in der einfachen Regression (Reduktion unserer Unsicherheit über das Kriterium)

\center
```{r out.height="62%", fig.height=4, fig.width=7}
cits.percent <- paste0(round(r2.cits*100),"%")
pubs.percent <- paste0(round(r2.pubs*100),"%")

par(mfrow=c(1,2))
venn("111+011",snames = c("Zitationen","Publikationen","Gehalt"))
venn.label("111+011", cits.percent)
venn("111+101",snames = c("Zitationen","Publikationen","Gehalt"))
venn.label("111+101",pubs.percent)
```

## Multiples $R^2$


\begincols
  \begincol{.48\textwidth}

  - Warum summieren sich die $R^2$ der einfachen Regression nicht auf?
  - Abhängigkeit (Kovarianz) der Prädiktoren führt zu gemeinsamer Information über das Kriterium, die nur einmal genutzt werden kann
  
\endcol
\begincol{.48\textwidth}
  
\center

```{r fig.height=4, fig.width=6}
cohen.venn("111+101+011")
venn.label("111+101+011", paste0(round(100*r2.total),"%"))
#title("CENN")
```

\endcol
\endcols


## Gemeinsame und einzigartige Effekte (hierarchische Regression)

\center
```{r, out.height="80%", fig.height=4, fig.width=6}


library(venn)
#venn("011+111") # Total of B in C
#venn("101+111") # Total of A in C
#venn("101") # Unique of A
#venn("011") # Unique of B
#venn("111") # Shared A and B
#venn("001") # Unique in C (unexplained by A and B)

venn("001",snames = c("Zitationen","Publikationen","Gehalt"))
venn.label("001",txt = paste0(round((1-r2.total)*100,0),"%"))
venn.label("111",txt=paste0(round(r2.joint*100,0),"%"))
venn.label("101",txt = paste0(round(cits.unique*100,0),"%"))
venn.label("011",txt = paste0(round(pubs.unique*100,0),"%"))
```
    
## Zusammenfassung

- Die Regression erlaubt uns den Zusammenhang mehrerer Variablen mit einer Kriteriumsvariable zu untersuchen
- Wir können Vorhersagen treffen
- Dazu betrachten wir die (partiellen) Regressionskoeffizienten und die Modellpassung (Bestimmtheitsmaß)

## Ausblick

- Statistische Tests für die Modellgüte und die Beiträge einzelner Variablen (bisher nur Effektstärken; *praktische Bedeutsamkeit* versus *statistische Bedeutsamkeit*)
- Modellannahmen (z.B. Korrektheit, Linearität, Homoskedastizität) und was tun, wenn diese verletzt sind
- Kodierung von kategorialen Variablen
- Interaktionen (Multiplikative Effekte)

## Zusatzmaterial

## Linearität

Residuen-Plot

```{r}
set.seed(234)
x <- seq()
N<-100
x <- seq(1:N)
y<-rnorm(N,(50-x)^2/1000,1)
ggplot(data=data.frame(x,y),aes(x=x,y=y))+geom_point()+
  xlab("Prädizierter Wert")+ylab("Residuen")

```

## Varianzgleichheit (Homoskedastizität)

```{r}
N<-100
x <- seq(1:N)
y<-rnorm(N,0, 1+8*(x/N))
ggplot(data=data.frame(x,y),aes(x=x,y=y))+geom_point()+
  xlab("Prädizierter Wert")+ylab("Residuen")
```

## Relative Wichtigkeit

Wie groß sind die Einflüsse relativ zu einander?

Die Regressionskoeffizienten sind zunächst für direkte Vergleiche (innerhalb des Modells) ungeeignet, da sie von der Streuung der Prädiktoren abhängen.

Bsp.: Die Einheit von $b_1$ ist $\frac{USD}{Publikation}$, die von $b_2$ ist $\frac{USD}{Zitation}$.

$\rightarrow$ Standardisieren

Allgemein: 
$$\beta_i=\frac{SD_{X_i}}{SD_{Y_i}} \cdot b_i $$


Interpretation: 
Veränderung im Kriterium, wenn man Prädiktor um eine Standardabweichung erhöht (kontrolliert für alle anderen Prädiktoren)

```{r}
std.lm <- lm(scale(salary) ~ scale(pubs)+scale(cits), data=cohen.dat)
```

## Standardisierte Partielle Regressionskoeffizienten

Rechenbeispiel:

- $SD_{X_1}=$ `r round(sd(cohen.dat$pubs),2)` (Publikationen)
- $SD_{Y}=$ `r round(sd(cohen.dat$salary),2)` (USD)
 
$$\beta_1=\frac{SD_{X_1}}{SD_{Y}} \cdot b_1 = \frac{14 \mathrm{\ Pub}}{9706 \ USD} \cdot 252 \frac{\mathrm{USD}}{\mathrm{Pub}} = 0.36 $$

In unserem Fall erhalten wir folgende (einheitenlose) standardisierten partielle Regressionskoeffizienten:

> Gehalt = 0+ `r round(coef(std.lm)[2],2)` $\cdot$ Publikationen + `r round(coef(std.lm)[3],2)` $\cdot$ Zitationen 

