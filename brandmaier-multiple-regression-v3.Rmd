---
title: "Multiple Regression"
subtitle: "Bachelor Psychologie, 2.FS"
author: "Andreas M. Brandmaier"
#institute: "Max Planck Institute for Human Development"
date: "12/3/2019"
output: 
  beamer_presentation:
    latex_engine: xelatex
    theme: metropolis
    includes:
      in_header: mystyle.tex
classoption: "aspectratio=169"

---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, out.height = "90%", cache=FALSE)
require(ggplot2)

mytheme <- ggthemes::theme_base()
#mytheme <- theme_minimal()


##Cohen (2003) Table 3.5.1
cohen.dat <- data.frame(
    salary = c(51876, 54511, 53425, 61863, 52926, 47034, 66432, 61100, 41934,
      47454, 49832, 47047, 39115, 59677, 61458, 54528, 60327, 56600,
      52542, 50455, 51647, 62895, 53740, 75822, 56596, 55682, 62091,
      42162, 52646, 74199, 50729, 70011, 37939, 39652, 68987, 55579,
      54671, 57704, 44045, 51122, 47082, 60009, 58632, 38340, 71219,
      53712, 54782, 83503, 47212, 52840, 53650, 50931, 66784, 49751,
      74343, 57710, 52676, 41195, 45662, 47606, 44301, 58582),
pubs =c(18,3,2,17,11,6,38,48,9,22,30,21, 10, 27, 37, 8, 13, 6, 12, 29, 29, 7, 6, 69, 11, 9, 20, 41, 3, 27, 14, 23, 1, 7, 19, 11, 31, 9, 12, 32, 26, 12, 9, 6, 39, 16, 12, 50, 18, 16, 5, 20, 50,
      6, 19, 11, 13, 3, 8, 11, 25, 4),
    cits = c(50, 26, 50, 34, 41, 37, 48, 56, 19, 29,
        28, 31, 25, 40, 61, 32, 36, 69, 47, 29, 35,
        35, 18, 90, 60, 30, 27, 35, 14, 56, 50, 25,
        35, 1, 69, 69, 27, 50, 32, 33, 45, 54, 47, 29,
        69, 47, 43, 55, 33, 28, 42, 24, 31, 27,
        83, 49, 14, 36, 34, 70, 27, 28),
sex= c(1,1,1,0,1,0,0,0,0,0,1,0,1,0,0,0,1,0,1,1,1,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,1,0,
  0,1,0,0,1,1,1,1,0,1,1,1,1,1)
)
```

## Was ist Regression?

Frage: Wie groß ist der Einfluss *mehrerer* metrischer oder kategorialer Prädiktoren auf ein metrisches Kriterium?

![](pathdiagram.pdf)

## Zentrale Fragestellungen

> - *Ursachenanalyse*: Gibt es einen Zusammenhang zwischen den Prädiktoren und der Kriteriumsvariable? Wie eng ist dieser?
> - *Wirkungsanalyse*: Wie verändert sich die Kriteriumsvariable bei einer Änderung der Prädiktoren?
> - *Prognose*: Können die Messwerte der Kriteriumsvariable durch die Werte der Prädiktoren vorhergesagt werden?

## Ein einfaches Vorhersagemodell

Ein einfaches Prognose-Modell: Wir sagen für jede Person den Mittelwert von Gehalt vorher (ohne die Prädiktoren zu kennen)

\center

```{r fig.height=3.5, fig.width=5, out.height="80%"}
lm.dumb<- lm(salary~1, data=cohen.dat)

ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
    geom_abline(intercept=coef(lm.dumb),slope=0,lwd=2,color="blue")+
  mytheme#+
  #ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")


```

## Konstante Regression

Ein formales Modell für die beobachteten Daten $i \in 1..N$ mit $N$ die Stichprobengröße:
$$ y_i = \color{blue}b_0 + \color{red}e_i$$

- $y_i$: Wert von Person $i$ im Kriterium
- $\color{blue}b_0$: Regressionskonstante 
- $e_i$: Vorhersagefehler für Person $i$ (Residuum)

## Ein verbessertes Vorhersagemodell

\center
```{r fig.height=3.5, fig.width=5}
lm.pubs <- lm(salary~pubs, data=cohen.dat)

lm.pubs <- lm(salary~pubs, data=cohen.dat)
df.test <- data.frame(pubs = 60)
df.test$salary <- predict(lm.pubs, df.test)

ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
    geom_smooth(method = "lm",se=FALSE,lwd=2)+
 #   geom_point(data=df.test,size=8,color='blue',pch=1)+
 # geom_segment(x=60,xend=60,y=0,yend=df.test$salary,lty=2,col="grey")+
#  geom_segment(x=0,xend=60,y=df.test$salary,yend=df.test$salary,lty=2,col="grey")+
  mytheme#+
  #ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")


```


## Einfache Regression

Einfache Regressionsgleichung für einen Prädiktor $X$ und die metrische Kriteriumsvariable $Y$:

\Huge$$ y_i = \color{blue}b_0\color{black} + \color{blue}b_1\color{black} \cdot x_i + \color{red}{e_i}$$\normalsize

- $y_i$: Wert von Person $i$ im Kriterium
- $x_{i,j}$: Wert von Person $i$ in dem Prädiktor $X$
- $e_i$: Vorhersagefehler für Person $i$ (Residuum)
- $\color{blue}b_0$: Regressionskonstante 
- $\color{blue}b_1$: Regressionskoeffizient


## Vorhersage

- Wir erhalten eine Modellvorhersage für einen neuen Punkt durch Einsetzen der Prädiktoren-Werte
- Unsere Regressionsgleichung lautet

$$y_i = 48439 + 351 \cdot x_i + e_i$$

- Für den Fehler nehmen wir an, dass er im Erwartungswert 0 ist (siehe Modellannahmen später) und somit erhalten wir

$$\hat{y}_i = 48439 + 351 \cdot 60 = 69487$$

## Interpretation der Koeffizienten

\begincols
\begincol{.38\textwidth}

- *Regressionskonstante*: Wert des Kriteriums, wenn der Prädiktor den Wert 0 annimmt.
- *Regressionskoeffizienten* (auch Regressionsgewichte): Erwartete Veränderung des Kriteriums $Y$, wenn
man den Prädiktor $X_i$ um eine Einheit erhöht.

\endcol
\begincol{.58\textwidth}

```{r fig.height=3.5, fig.width=5}
lm.pubs <- lm(salary~pubs, data=cohen.dat)

lm.pubs <- lm(salary~pubs, data=cohen.dat)
df.test <- data.frame(pubs = 0)
df.test$salary <- predict(lm.pubs, df.test)
df.test2 <- data.frame(pubs = 20)
df.test2$salary <- predict(lm.pubs, df.test2)
ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
  geom_smooth(method = "lm",se=FALSE)+
  geom_point(data=df.test,size=8,color='blue',pch=1)+
  geom_segment(x=20,xend=20,y=coef(lm.pubs)[1],yend=df.test2$salary,lty=2,col="blue")+
  geom_segment(x=0,xend=20,y=coef(lm.pubs)[1],yend=coef(lm.pubs)[1],lty=2,col="blue")+
  mytheme#+
#  ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")
```
\endcol
\endcols



## Beispiel

Wie gut ist unsere Regressionsgerade überhaupt? Dazu betrachten wir die Fehler in den beobachteten Werten:

\center

```{r fig.height=3, fig.width=5, out.height="75%",dpi=180}
id <-45
lm.pubs <- lm(salary~pubs, data=cohen.dat)
ymin <- predict(lm.pubs)[id]
ymax <- cohen.dat[id,"salary"]

ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point()+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
  geom_smooth(method = "lm",se=FALSE)+
  geom_linerange(lty=2,x=cohen.dat[id,"pubs"],ymin=ymin,ymax=ymax,col="red")+
  geom_text(x=cohen.dat[id,"pubs"]+5,y=(ymax+ymin)/2+5000,label="Fehler",col="red")+
  mytheme


```

## Schätzung: Kleinste Quadrate Methode

- Idee: Das beste Regressionsmodell soll das sein, das die Streuung der Fehler minimiert

- Wir nennen das die Residuenquadratsumme oder auch Summe der quadrierten Residuuen (SQR)

$$\sum^n_{i=1} (e_i-\bar{e}_i)^2 = \sum^n_{i=1}{e^2_i}=SQR$$

- Wähle die Koeffizienten so, dass wir die Summe der quadrierten Fehlerterme  minimieren

$$ \sum_{i=1}^n{} e^2_i = \sum_{i=1}^n \left(y_i - b_0 + b_1 \cdot x_i \right) $$

- Exakt und effizient lösbar (falls lösbar)

- Konform mit einem Gaußschen Fehlermodell: Die Störgrößen sind normalverteilt

## Kleinste Quadrate Methode

\center

```{r out.height="80%",fig.height=4, fig.width=6}
plot_kq <- function(intercept=NULL, slope=NULL) {
  
model <- lm(salary~pubs, data = cohen.dat)
if (!is.null(intercept)) { model$coefficients[1]<-intercept }
if (!is.null(slope)) { model$coefficients[2]<-slope }

scf <- sqrt(var(cohen.dat$salary))/sqrt(var(cohen.dat$pubs))

y.hat<-predict(model)
empty <- rep(NA,nrow(cohen.dat)*4)
df <- data.frame(x=empty,y=empty,grp=empty)
for (i in 1:nrow(cohen.dat)) {
  dist <- y.hat[i]-cohen.dat[i,"salary"]
  disty <- dist
  distx <- dist / scf
  y <- cohen.dat[i,"salary"]
  x <- cohen.dat[i,"pubs"] 
 
  df[ (i-1)*4+1, ] <- c(x,y,i)
  df[ (i-1)*4+2, ] <- c(x-distx,y,i)
  df[ (i-1)*4+3, ] <- c(x-distx,y+disty,i)  
  df[ (i-1)*4+4, ] <- c(x,y+disty,i)  
}
ggplot(cohen.dat, aes(x=pubs,y=salary))+
  
 # geom_smooth(method='lm',se=FALSE)+
 #fillcolor<-'red' #'#FFC43F'
  geom_polygon(data=df,aes(x=x,y=y,group=grp),alpha=.20, fill='red',color='#FFFFFF')+
  geom_point(data=cohen.dat, aes(x=pubs,y=salary))+
   geom_abline(intercept=coef(model)[1],slope=coef(model)[2])+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
  mytheme+ ggtitle("Einfache Regression",subtitle="Abhängige Variable: Gehalt")
}

(plot_kq())
```

## Kleinste-Quadrate Schätzer

\center

```{r out.height="80%",fig.height=4, fig.width=6}
plot_kq(intercept=68000, slope=100)
```

## Multiple Regression

Frage: Wie groß ist der Einfluss *mehrerer* metrischer oder
kategorialer Prädiktoren auf ein metrisches Kriterium?

$$\color{black}\Huge y_i = \color{blue}b_0 + \color{blue}b_1\color{black} \cdot x_{i,1} +\color{blue} b_2\color{black} \cdot x_{i,2} + \ldots + \color{blue}b_k \color{black} \cdot x_{i,k}+\color{red} e_i$$

- $y_i$: Wert von Person $i$ im Kriterium
- $x_{i,j}$: Wert von Person $i$ in dem Prädiktor $j$
- $e_i$: Vorhersagefehler für Person $i$ (Residuum)
- $b_0$: Regressionskonstante (Intercept)
- $b_j$: Regressionsgewichte ($j=1\ldots j$)

## Beispiel

\center

```{r  fig.height=4, fig.width=6, out.width="85%", out.height="85%"}
library(scatterplot3d) # This library will allow us to draw 3d plot

cohenplot <- function() {
  
  temp <- cohen.dat 
  temp$salary <- temp$salary/1000
  
  plot3d <- scatterplot3d(temp$pubs, temp$cits, temp$salary,angle=55, scale.y=0.7, pch=16,
                        xlab="X: Publikationen", ylab="Y: Zitationen", zlab="Z: Gehalt [T-USD]")

  my.lm<- lm(salary ~ pubs + cits, temp)
  plot3d$plane3d(my.lm, lty.box = "solid")
  return(plot3d)
}


my.lm<- lm(salary ~ pubs + cits, cohen.dat)
plot3d <- cohenplot()

```

## Regressionskoeffizient


\begincols
  \begincol{.38\textwidth}
  
> Gehalt = `r round(coef(my.lm)[1],2)`+ `r round(coef(my.lm)[2],2)` $\cdot$ Publikationen + `r round(coef(my.lm)[3],2)` $\cdot$ Zitationen 

$b_0$ gibt an, welcher Wert für die deterministische Komponente der Untersuchungsvariablen zu erwarten ist, falls sämtliche exogenen Variablen den Wert Null realisieren.


  \endcol
\begincol{.58\textwidth}

```{r message=FALSE, echo=FALSE, warning=FALSE, results=FALSE}
  
  temp <- cohen.dat 
  temp$salary <- temp$salary/1000
  
  pdf("temp-3d.pdf", width=5, height=5)
  
  plot3d <- scatterplot3d(temp$pubs, temp$cits, temp$salary,angle=55, scale.y=0.7, pch=16,
                        xlab="X: Publikationen", ylab="Y: Zitationen", zlab="Z: Gehalt [T-USD]")

  my.lm<- lm(salary ~ pubs + cits, temp)
  plot3d$plane3d(my.lm, lty.box = "solid")
plot3d$points3d(0,0,coef(my.lm)[1],col="red",pch=16,cex=3)

dev.off()
```

![](temp-3d.pdf)

\endcol
\endcols



## Partielle Regressionskoeffizient

- Die $b_i$ geben an, um wieviel sich der Wert von dem Kriterium ($y$) ändert, wenn man den zugehörigen $i$-ten Prädiktor um 1 Einheit verändert *und* man alle übrigen Prädiktoren konstant hält.

- Daher bezeichnen wir diese nun als _partielle Regressionskoeffizienten_


## Modellvorhersagen mit der Partiellen Regression

```{r fig.height=4, fig.width=6}
my.lm<-lm(salary~pubs+cits,cohen.dat)

#my.lm.pubs <- lm(salary~pubs, cohen.dat)
coefs1 <- coef(my.lm)
coefs2 <- coefs1+c(20*coef(my.lm)[3],0,0)
coefs3 <- coefs1+c(40*coef(my.lm)[3],0,0)
ggplot(cohen.dat, aes(x=pubs,y=salary))+geom_point(col='white')+
  xlab("Anzahl Publikationen")+ylab("Gehalt [USD]")+
  geom_abline(intercept=coefs1[1], slope=coefs1[2])+
  geom_abline(intercept=coefs2[1], slope=coefs2[2])+
    geom_abline(intercept=coefs3[1], slope=coefs3[2])+
  #geom_abline(intercept=coefs4[1], slope=coefs4[2])+
  geom_text(x=30,y=45000,label="<- 0 Zitationen")+
  geom_text(x=50,y=55000,label="<- 20 Zitationen")+
  geom_text(x=45,y=65000,label="40 Zitationen ->")+
  mytheme
```


## Relative Wichtigkeit

Wie groß sind die Einflüsse relativ zu einander?

Die Regressionskoeffizienten sind zunächst für direkte Vergleiche (innerhalb des Modells) ungeeignet, da sie von der Streuung der Prädiktoren abhängen.

Bsp.: Die Einheit von $b_1$ ist $\frac{USD}{Publikation}$, die von $b_2$ ist $\frac{USD}{Zitation}$.

-> Standardisieren

Allgemein: 
$$\beta_i=\frac{SD_{X_i}}{SD_{Y_i}} \cdot b_i $$


Interpretation: 
Veränderung im Kriterium, wenn man Prädiktor um eine Standardabweichung erhöht (kontrolliert für alle anderen Prädiktoren)

```{r}
std.lm <- lm(scale(salary) ~ scale(pubs)+scale(cits), data=cohen.dat)
```

## Standardisierte Partielle Regressionskoeffizienten

Rechenbeispiel:

- $SD_{X_1}=$ `r round(sd(cohen.dat$pubs),2)` (Publikationen)
- $SD_{Y}=$ `r round(sd(cohen.dat$salary),2)` (USD)
 
$$\beta_1=\frac{SD_{X_1}}{SD_{Y}} \cdot b_1 = \frac{14 \mathrm{\ Pub}}{9706 \ USD} \cdot 252 \frac{\mathrm{USD}}{\mathrm{Pub}} = 0.36 $$

In unserem Fall erhalten wir folgende (einheitenlose) standardisierten partielle Regressionskoeffizienten:

> Gehalt = 0+ `r round(coef(std.lm)[2],2)` $\cdot$ Publikationen + `r round(coef(std.lm)[3],2)` $\cdot$ Zitationen 

## Modellgüte

- Die Summe der quadrierten Residuuen (SQR) ist ein Maß für die Modellgüte 
- Je kleiner, desto besser die Passung des Modells zu den Daten
- Auch hier gilt, SQR ist abhängig von der Einheit der Kriteriumsvariable (hier, $USD^2$)
- Standardisieren!

## Erklärte Varianz

Varianz: Maß für die Unsicherheit einer normalverteilten Variable

> Varianz der Kriteriumsvariable: Die erwartete (quadrierte) Abweichung vom Mittelwert. Wenn wir die Kriteriumsvariable raten müssten, wäre das der erwartete (quadrierte) Fehler

- Wir standardisieren die Streuung der Fehler (Unsicherheit trotz des Modells) anhand der Streuung der Kriteriumsvariable (Unsicherheit ohne Modell) und erhalten ein prozentuales Maß der erklärten Varianz (oder: aufgeklärten Unsicherheit)


## Bestimmtheitsmaß

Determinationskoeffizient  / Bestimmheitsmaß (Multiples $R^2$):
  
  $$ R^2 = \frac{\mathrm{ durch\  alle\ Prädiktoren\ in \ }Y\ \mathrm{erklärte\ Varianz }}{\mathrm{Varianz\ von\ }Y} $$

- Berechnung der erklärten Varianz als $$1- \frac{1}{n-1} \cdot RSQ $$
- Je höher, desto besser!

  
## Venn-Diagramm der Varianzen

```{r}
# only code --- no graphics output
library(venn)
cohen.venn <- function(x, ...) {
  venn(x,snames = c("Zitationen","Publikationen","Gehalt"), cexsn=1,...)
}

venn.label <- function(x, txt) {
  centroid <- getCentroid(getZones(x))[[1]]
  text(centroid[1], centroid[2], labels = txt, cex = 0.85)
}
my.lm<- lm(salary ~ pubs + cits, cohen.dat)
smlm <- summary(my.lm)

r2.total <- smlm$r.squared # 42% explained

my.lm.pubs <- lm(salary ~ pubs, cohen.dat)
r2.pubs <- summary(my.lm.pubs)$r.squared # 25%


my.lm.cits <- lm(salary ~ cits, cohen.dat)
r2.cits <- summary(my.lm.cits)$r.squared # 30%

cits.unique <- r2.total-r2.cits
pubs.unique <- r2.total-r2.pubs
r2.joint <- r2.total-pubs.unique-cits.unique
```

Schatierte Fläche: Die gesamte Varianz des Kriteriums (unsere Unsicherheit über das Kriterium)

```{r fig.height=4, fig.width=6, out.height="75%"}
cohen.venn("001+111+101+011")
```

## Venn-Diagramm der einfachen $R^2$

- Schatierte Fläche: Aufgeklärte Varianz in der einfachen Regression (Reduktion unserer Unsicherheit über das Kriterium)
- Das Verhältnis dieser und der gesamten Fläche des Kriteriums entspricht dem Bestimmtheitsmaß $R^2$.

\center
```{r out.height="62%", fig.height=4, fig.width=6}
cits.percent <- paste0(round(r2.cits*100),"%")
pubs.percent <- paste0(round(r2.pubs*100),"%")

par(mfrow=c(1,2))
venn("111+011",snames = c("Zitationen","Publikationen","Gehalt"))
venn.label("111+011", cits.percent)
venn("111+101",snames = c("Zitationen","Publikationen","Gehalt"))
venn.label("111+101",pubs.percent)
```

## Multiples $R^2$


\begincols
  \begincol{.48\textwidth}

  - Warum summieren sich die $R^2$ der einfachen Regression nicht auf?
  - Abhängigkeit der Prädiktoren führt zu gemeinsamer Information über das Kriterium, die nur einmal genutzt werden kann
  
\endcol
\begincol{.48\textwidth}
  
\center

```{r fig.height=4, fig.width=6}
cohen.venn("111+101+011")
venn.label("111+101+011", paste0(round(100*r2.total),"%"))
#title("CENN")
```

\endcol
\endcols


## Gemeinsame und einzigartige Effekte

\center
```{r, out.height="80%", fig.height=4, fig.width=6}


library(venn)
#venn("011+111") # Total of B in C
#venn("101+111") # Total of A in C
#venn("101") # Unique of A
#venn("011") # Unique of B
#venn("111") # Shared A and B
#venn("001") # Unique in C (unexplained by A and B)

venn("001",snames = c("Zitationen","Publikationen","Gehalt"))
venn.label("001",txt = round(1-r2.total,2))
venn.label("111",txt=round(r2.joint,2))
venn.label("101",txt = round(cits.unique,2))
venn.label("011",txt = round(pubs.unique,2))
```

    
## Zusammenfassung

- Die Regression erlaubt uns den Zusammenhang mehrerer Variablen mit einer Kriteriumsvariable zu untersuchen
- Wir können damit Ursachen und Wirkungen analysieren, sowie Vorhersagen treffen
- Dazu betrachten wir die (partiellen) Regressionskoeffizienten und die Modellpassung (Bestimmtheitsmaß)

## Ausblick

- Statistische Tests für die Modellgüte und die Beiträge einzelner Variablen (bisher nur Effektstärken)
- Modellannahmen und was tun, wenn diese verletzt sind



